---
title: "Analyse Bivariée "
author: "PC"
date: '`r format(Sys.Date(), " %d %B  %Y")`'
output: ioslides_presentation
css : style.css
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width =6,fig.align="center", fig.height = 3)
library(knitr)
library(xtable)
```

# Introduction générale 

## Sources et Reférences

- Ce cours reprend le cheminement du cours M2 IGAST 2017 d'Élodie Buard 
- R et espace https://framabook.org/r-et-espace/
- Probabilités, analyse de données et statistiques , Gilbert Saporta, Editions TECHNIP, 2011
- Nombreuses ressources en ligne : 
   http://www.foad-mooc.auf.org/IMG/pdf/424B_-Application_des_methodes_statistiques_d_analyse.pdf
- et wikipedia !

## Deux familles statistiques

### Statistiques **inférentielles**
  Pour répondre à la question : «A partir d'un échantillon , que peut-on attendre (inférer) de la population ?»
  
  - Modèles, estimateurs, ... : **régression**, **estimation**, **extrapolation**
  - *Liaisons* statistiques : **corrélation**, **covariance**
  - test statistiques, notions de probabilités
  - e.g. sondages, rencensement, intervalle de confiance , prédictions, ...

## Deux familles statistiques
  
### Statistiques **descriptives** 

  Pour **résumer**, **synthétiser**, rendre intelligible, les propriétés d'une **population** à partir des **variables** qui décrivent les individus et la **répartition** de leurs valeurs.
  
  - Graphiques (histogrammes, boxplots, )
  - Mesures (fréquences , distributions , moments)
  - classification, ACP,...

## Vocabulaire 
 
### Population
 **Ensemble** d'individus  
 
 "données", "corpus", "échantillon", "data"
 
### Individu
 Unité statistique **élémentaire**: un individu, 
 
 
 "les lignes du tableau"

## Vocabulaire 

### Variables 
 **Caractéristiques, propriétés** d’un individu, __mesurées__ par des enquêtes, des observations...

"les colonnes du tableau"  

- **Qualitatives** : facteurs, classes , catégories,  e.g. Sexe, CSP, type ; on parle de **modalité**
 
- **Quantitatives** : nombres e.g. Âge, taille, revenu, surface



<span style="color:red">&#9888;</span> **Valeurs** et **Nature** de données sont deux choses distinctes. 
<center> 3≠trois≠III </center>



 <span style="color:red">&#9888;</span> Valeurs manquantes  
 <center> NA , NaN, Null </center>





## L'objet du cours : l' Analyse Bivariée

Objectif : **Analyser**  le **lien** entre deux variables

- **2 variables quantitatives**

<font size=4>
"nombre d'habitants et nombre de lignes de bus  par département"

"nombre de lignes de bus en 1998 et en 2018"  
</font>

- **2 variables qualitatives** 

<font size=4>
"couleur des yeux  et port de lunettes" 
</font>

- **Une variable quantitative, une variable qualitative** 

<font size=4>
"taille et couleur des yeux"  
</font>


## Mise en garde

<span style="color:red; font-size:100%">&#9888; </span> Une liaison, même très forte, entre deux variables, <span style="color:red">n'indique pas la causalité! &#9888;</span>

Erreur très courrante, très tentante.

<center>
<img src="chart.svg", width=100%></img>
</center>
© TylerVigen http://tylervigen.com/spurious-correlations




## Analyse bivariée avec des données spatiales

### Données "non spatiales"" 

Les variables sont des valeurs attributaires d'individus, leurs localisation n'est pas prise en compte

### Données spatiales
  
  Individus restreints spatialement (selection spatiale), ou variables de provenance géographique (e.g. Lieu de résidence) renseignées pour les individus 
  
  quid des **distances** ? $\rightarrow$ modèle gravitaire , réseau etc.

### Données localisées

  Auto-corrélation spatiale (Moran's I)
  Geographicaly Weightd Regression (GWR)


# Lien entre deux variables quantitatives



## Première étape   

<span style="color:red">Toujours en premier:</span> Regarder l'aspect des données avec des graphiques ("exploration visuelle") 

```{r plotIris, fig.width=8}
data(iris)
plot(iris)
```

Existe-t-il un lien entre des variables ?



## Forme des dépendances 

```{r formes, echo=FALSE, fig.width= 8, fig.height=5}
x <- runif(100, min = 0, max=10)
y1 <- 8.5*x - 23 + rnorm(100,0, 12)
y2 <- -16*x +12 + rnorm(100,0,10)
y3 <- sqrt(exp(x)) + rnorm(100,0,8)
y4 <-  50*sin(x) + rnorm(100,0,10)
y5 <-  rnorm(100, 0, 60)
y6 <- 21.345
data2 <-  data.frame(x, y1, y2, y3, y4, y5, y6)
library(reshape2)
library(ggplot2)
data2 <- melt(data2, idvars=c("x"), measure.vars=c("y1","y2","y3","y4", "y5", "y6"))
plo2 <-  ggplot(data2, aes(x=x, y=value))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  facet_wrap(~variable)+
  xlab("value of x variable")+
  ylab("value of yi variable")
  
plo2
```


## Les étapes
1. Tracer le nuage de points
2. Existe-t-il une liaison ? 
3. Est-elle de forme linéaire ?  De quel sens ?
4. Si la liaison est de forme linéaire $\rightarrow$ faire une **régression** 
5. Si la liaison est non linéaire, est-elle monotone ?  De forme connue ?$\rightarrow$  Proposer un **modèle**


5bis. Tenter un modèle **LOESS** avec prudence (uniquement descriptif , aucun pouvoir de généralisation) cf le blog de Lise Vaudor [http://perso.ens-lyon.fr/lise.vaudor/regression-loess/]

## Régression linéaire 
 
Si la forme du nuage de points s'y prête, on peut faire une **régression linéaire** (ou **ajustement** linéaire).
 
On cherche la droite qui «passe au mieux» (=*ajustée*) dans le nuage de points formés par le tracé de deux variables quantitatives $Var1$ et $Var2$ qui permet de vsiualiser: 

* l'**intensité** du lien / de la dépendance  : points proche de la droite ou non ?
* la **forme** de la dépendance : linéaire ou non ?
* le **sens** de la dépendance  : nulle, positive ou négative ?

## Régression linéaire 

```{r regline, echo=FALSE, fig.width=6, fig.height=4}
library(ggplot2)
data <-  data.frame(X=c(1,2,3,5,6,7,12,14,22, 2,3 ,10,10,9,7),Y=c(15,16,13, 9,9,12,3,2,5,11,12,7,8,9,8))
plo <-  ggplot(data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  xlab("Var1")+
  ylab("Var2")
plo
```

 
 
«Droite qui passe au mieux»  = qui minimise la **somme des écarts quadratiques** entre la droite et les points du nuage.




 
## Régression linéaire : le modèle
 

L'**équation** de la droite  est un **modèle linéaire** de la relation statistique qui lie $V1$ et $V2$; on dit que $V1$ **explique** $V2$ :

Notre modèle : 
<center>
$V2=aV1+b$
</center>

 
\

Si la régression linéaire est correcte, alors pour un individu $i$ dont on connait $V1_i$, on infère la valeur $V2_i$ par le modèle :    $V2_i = a \times V1_i +b$
 
 
  $\implies$Comment déterminer qu'une régression linéaire est «correcte» ? 
 
## le $R^2$
 
 $R^2 \in [0;1]$ , coefficient de détermination linéaire .

Donne la **qualité de prédiction** de la régression. 

"Proche de 1"" $\equiv$ "très bonne qualité"
 
C'est le pourcentage de variation de $V_2$ expliqué par la variation de $V_1$

C'est aussi le carré de la corrélation $cor(V_1,V_2)$ (celle donnée par le $r$ de Pearson)
  
  $R^2  = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$


## Format des resultats de `lm` avec R

```{r reslm,fig.width=6, fig.height=4}
summary(lm(iris$Petal.Length~iris$Petal.Width))
```

Distribution des résidus, coefficients du modèle ajusté et leur **p-value** associée (ici sur un test de Student, notée `Pr(>|t|)` et  $R^2$) 


## Comment evaluer la "validité" du modèle linaire ?

En pratique, il faut réunir deux critères : 

- des coefficients avec des p-values associées faibles (e.g. <0.05)  $\leftrightarrow$ "on a peu de chances de se tromper"
- un $R^2$ élevé  $\leftrightarrow$ "le modèle prédit bien les observations"


## la p-value

Elle peut s'interpréter comme la probabilité d'avoir un résultat identique tout en conservant l'hypothèse nulle. 

Ici , l'hypothèse nulle est "les deux séries sont indépendantes". 

On peut grossièrement considérer que p-value = pourcentage de chances de se tromper en rejetant l'hypothèse nulle, c'est à dire en considérant que  les deux séries ne sont pas indépendantes et qu'il existe une relation (ici, linéaire car nous testons le coefficient de coorélation linéaire) entre les deux.


Rappel : 
  - conserver $H_0$ : considérer les deux variables comme indépendantes
  - rejeter $H_0$ : considérer les deux variables comme ayant une relation statistique, un lien.



## Bonus: Critères de significativité du lien linéaire 

Les **résidus** $\epsilon_i$ (écart entre valeur observée et valeur prédite  par le modèle pour l'individu $i$) doivent:

- être indépendants  : $cov(x_i, \epsilon_i) = 0$
- être distribués selon un loi normale de moyenne nulle $\mathscr{N}(0,\sigma_{\epsilon})$
- être distribués de façon homogène (homoscédasticité), i.e. de variance constante $var(\epsilon_i)=\sigma_{\epsilon}^2$ , indépendante de l'observation



## Bonus: Evaluation de l'indépendance des résidus avec R 

Graphique de la fonction `acf`:  Si une barre exceptée la première dépasse  la ligne en pointillés, on peut remettre en cause l'indépendance des résidus. Ici, c'est le cas.

```{r residualSigni3,fig.width=6, fig.height=4}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
acf(residuals(modele1))
```



 


## Bonus:Les 4 graphiques résultats de la fonction `lm`


La fonction `lm` de R et ses résultats permettent de tracer 4 graphiques pour évaluer certains des critères de significativité.


```{r residualSignif,fig.width=6, fig.height=4, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
par(mfrow=c(2,2)) # pour avoir une matrice de grpahes
plot(modele1)
```



## Bonus: Évaluer l'homogénéité des résidus avec R

Premier graphique : pour vérifier que le nuage de points est homogène (e.g. pas de relation non-linéaire entre résidus et valeurs prédites)

Une éventuelle relation non-linéaire pourrait se retrouver dans les résidus.

```{r residualSignif1,fig.width=6, fig.height=3, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,1)
```

Ici : légère structure parabolique 

## Bonus: Evaluer la normalité de la distribution des résidus avec R

Le deuxième graphique "Q-Q plot" : pour vérifier l'**hypothèse de normalité des résidus**, les points doivent être proches de la bissectrice  


```{r residualSignif2,fig.width=6, fig.height=4, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,2)
```




## Bonus: Evaluer l'homoscédasticité des résidus 

<span style="font-size:18px;"> Le troisème graphique "Scale location" : si les résidus sont distribués de façon homogène suivant les valeurs "fittées",  alors la droite est plutôt horizontale et les points sont disposés de façon homogène autour.</span>

```{r residualSignif3,fig.width=6, fig.height=2.5, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,3)
```

<span style="font-size:18px;">Ici: légère pente mais les points sont distribués de façon relativement homogène autour de la droite.</span>



## Corrélation de deux variables avec R
Pour étudier l'intensité d'un lien statistique linéaire entre 2 variables
```{r corIris}
cor(iris$Petal.Length, iris$Petal.Width)
```

`cor(x,y)` donne le **coefficient de corrélation** $\in [-1;1]$ entre deux variables x et y (e.g. colonnes d'un dataframe) .

- +1 : les deux variables croissent ou décroissent conjointement
- -1 : quand l'une des variables croît, l'autre décroît.
- 0 : pas de relation **linéaire** entre les deux variables
 
 <font size="3">R donne le coefficient de Pearson par defaut, l'argument `method` de la fonction `cor()` permet de spécifier deux autres coefficients : Kendall et Spearman.</font>


## Test de corrélation entre deux variables avec R

Version plus complète : c'est un **test**, on a plusieurs indicateurs statistiques sur ce test, notamment la **p-value** et **l'intervalle de confiance**

```{r cortestIris}
cor.test(iris$Petal.Length, iris$Petal.Width)
```
Rappel : la p-value quantifie la **significativité** du test.
En général, on considère le test significatif si elle est en dessous de 5%, soit 0.05.  


## Calcul direct du coefficient de corrélation

Soient deux variables $V_1$ et $V_2$

Le coefficient de corrélation $r$ de $V_1$ et $V_2$ est la normalisation de la covariance par le produit des écart-types des variables 

$r= \frac{cov(V_1,V_2)}{\sigma_{V_1}\sigma_{V_2}}$ 




Rappel : La covariance est la moyenne du produit des écarts à la moyenne

$cov(V_1,V_2)= E[(V_1-E[V_1])(V_2-E[V_2])]$





## Matrice de corrélations


```{r corMatIris}
cor(iris[,1:4])
```

Présentation des corrélations entre les variables quantitatives d'un tableau, pour tous les couples de variables.

La matrice de corrélation est symmétrique, et sa diagonale est constituée de 1.




## Sensibilité aux 'outliers'

```{r sensitiveOut1}
X <-  c(3,2,3,4,1,2,3,4,5,2,3,4,3)
Y <-  c(1,2,2,2,3,3,3,3,3,4,4,4,5)
plot(X, Y, xlim = c(0,16), ylim= c(0,16))
cor.test(X,Y)$estimate
```


## Sensibilité aux 'outliers'

```{r sensitiveOut2}
X <-  c(3,2,3,4,1,2,3,4,5,2,3,4,3,15)
Y <-  c(1,2,2,2,3,3,3,3,3,4,4,4,5,15)
plot(X, Y, xlim = c(0,16), ylim= c(0,16))
cor.test(X,Y)$estimate
```

## Sensibilité aux 'outliers'

Outlier : observation "*anormale*",  par sa valeur extrème , comparée aux autres.


La corrélation et la régression linéaire sont très sensibles aux outliers.

$\rightarrow$ s'interroger sur la nécessité de nettoyer/filter les données et des conséquences 



## Regression linéaire avec R

Fonction  `lm()` , modèle de la forme `réponse ~ termes` 

```{r regress, echo=TRUE }
my_model <- lm(Petal.Width~Petal.Length, iris)
summary(my_model)
```


## Que faire lorsque la relation n'est pas linéaire ?

Quand les deux variables sembles corrélées , de façon **monotone** mais **non linéaire**,

$\rightarrow$ Coefficient de **Spearman**, basé sur le **rang** des individus.

<center>
$\rho = 1 - \frac{6\sum_{i=1}^{n}(rg(X_i)-rg(Y_i))^2 }{n^3 -n}$
</center>


avec :

$rg(X_i)$ le *rang* de $X_i$  (le classement de sa valeur) dans la distribution de $X$

$n$ le nombre d'individus

## Obtenir le coefficient Spearman avec R


```{r rho}
cor.test(iris$Sepal.Length, iris$Sepal.Width, method="spearman", exact = FALSE)
```


l'argument `exact` doit être précisé en cas de valeurs ex aequo dans les données.  


## Utilisation conjointe des coefficients de Pearson et Spearman 

$r$ (Pearson) et $\rho$ (Spearman) sont deux moyens d'estimer la corrélation: lequel choisir ?

si $r = \rho$: on garde $r$ (plus simple)

si $r < \rho$: la relation est non-linéaire : prendre  $\rho$

si $r > \rho$: il y a un biais, prendre $\rho$ (plus robuste)

... et toujours **tracer le nuage de points** pour examiner  la nature de la relation.


# Linéariser des relations non-linéaires


## Relation log-linéaire 

```{r loglin, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- 1.5*x^2.2  + rnorm(1000,10, 12)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```


Relation de type $y=ax^b$ 

se linéarise par $ln(y)=aln(x) + ln(b)$



## Relation géométrique (exponentielle) 

```{r expolin, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- exp(1.5*x+ 0.2)  + 10000*rnorm(1000,10, 12)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```


Relation de type $y=e^{ax+b}$ 

se linéarise par $ln(y)=ax + ln(b)$

## Relation logarithmique

```{r logar, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- 1.5*log(x)+ 0.2  + 0.22*rnorm(1000,1, 2)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```

Relation de type $y=a*ln(x)+b$ 

$\rightarrow$ changement de variable !

## Relation logistique

```{r logistic, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
ymax <-  10
ymin <- 0
y1 <- ymin+ ((ymax -ymin) /(1 + exp(-1.5*x+6.8))) + 0.5*rnorm(1000,1,1)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```

Relation de type $y= y_{min} * \frac{y_{max}-y_{min}}{1+e^{ax+b}}$

se linéarise par $ln\bigg(\frac{y_{max}-y}{y-y_{min}}\bigg)=ax+b$

# Lien entre deux variables qualitatives

## Représentation graphique

Pour deux variables qualitatives, on ne peut pas produire de nuages de points, ni de droite de régression.

$\rightarrow$ on peut représenter la table de contingence (cf. fonction `mosaicplot` de R).

```{r mosaic, echo=FALSE, fig.height=3}
library(RColorBrewer)
mypalette <-  brewer.pal(12,"Set3")
par(mar=c(0,0,0,0))
mosaicplot(~Class+Survived, data=Titanic, color=mypalette,main = "")
``` 


## Test statistique dit du "Chi 2"" ou "Chi carré""

Le test du $\chi ^2$ est un test d'indépendance, il mesure l'**écart**, la différence,  entre deux distributions de **variables qualitatives**  

Il répond à la question : "Existe-t-il un lien statistique entre deux séries de valeurs qualitatives"  

(La réponse est de type  OUI/NON , le $\chi^2$ ne donne pas l'**intensité** du lien)


## Test statistique dit du "Chi 2"" ou "Chi carré""

- Hypothèse nulle $H_0$ : les deux distributions sont indépendantes.
- «faire le test» permet de conserver ou de rejeter cette hypothèse

Principe: 

- On génère une **distribution théorique** à laquelle on va comparer la **distribution observée**.
- Cette distribution théorique reflête ce qui se passerait si on suppose que $H_0$ est vraie 
- Avec cette comparaison,  on pourra rejeter ou conserver l'hypothèse nulle.

La construction de cette distribution se fait à partir du **tableau de contingence**


##  Tableau de contingence

C'est un tableau à double entrée qui croise deux **variables qualitatives**. 


Dans une case on trouve l'effectif des individus caractérisés par la conjonction des modalités en ligne et en colonnes.


Exemple sur des formes géométriques de couleurs :

$$\begin{array}{c|c|c}
   & blanc & noir \\ 
   \hline
carré &  22 &   12 \\ 
  \hline
  rond &   10 &  30 \\ 
    \hline
  triangle &  26 &   5 \\ 
\end{array}$$


Dans R : fonction `table`

## Construction de la distribution théorique.

On commence par sommer les effectifs selon les modalités (en ligne et en colonne)

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  22 &   12 & 34\\ 
  \hline
  rond &   10 &  30 &  40  \\ 
    \hline
  triangle &  26 &   5 &  31\\
  \hline
  \texttt{total} & 58 & 47 & 105
  \end{array}$$

On appelle les sommes en lignes et en colonnes **sommes marginales**, elles sont mises dans les "marges" du tableau.



## Construction de la distribution théorique

En divisant par la taille de l'effectif, on obtient les **fréquences observées**.

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.20952381&  0.11428571 &  0.3238095\\ 
  \hline
  rond &    0.09523810 & 0.28571429 & 0.3809524  \\ 
    \hline
  triangle & 0.24761905 & 0.04761905 &   0.2952381\\
  \hline
  \texttt{total} &  0.552381 & 0.447619 & 1
  \end{array}$$


On obtient les **pourcentages de l'effectif** dans les cases du tableau.


C'est également la **probabilité** dans cette population, qu'un individu soit caractérisé par les modalités en ligne et en colonne.


## Construction de la distribution théorique

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.20952381&  0.11428571 &  \textbf{0.3238095}\\ 
  \hline
  rond &    0.09523810 & 0.28571429 & \textbf{0.3809524}  \\ 
    \hline
  triangle & 0.24761905 & 0.04761905 &   \textbf{0.2952381}\\
  \hline
  \texttt{total} &  \textbf{0.552381} & \textbf{0.447619} & 1
  \end{array}$$


De la même façon, les **fréquences marginales**, donnent la **probabilité** d'observer un individu de la modalité correspondant à la ligne ou à la colonne considérée.

<font size="4">
Exemple : dans cette population , j'ai 29.5% de chances de tirer un triangle
</font>

## Construction de la distribution théorique

Rappel : Probabilité conjointe de deux évènements $A$ et $B$ **indépendants**  
$P(A \cap B) = P(A) \times P(B)$

A partir des **fréquences marginales** précédentes, on obtient pour chaque couple de modalités la probabilité **théorique**,  celle qui suppose $H_0$, i.e. l'indépendance des deux variables, par un simple produit.

<font size="4">
Exemple :  Si $H_0$ est vraie, la probabilité d'observer un triangle noir est donnée par: 

$P(triangle \cap noir) = P(triangle) \times P(noir)$

$P(triangle \cap noir) =0.447619 \times 0.2952381  = 0.1321542$

La probabilité théorique d'observer un triangle noir est de 13,2%  
</font>

## Construction de la distribution théorique

On crée un second tableau, dont chaque case vaut le produit des fréquences marginales calculées sur le tableau des observations.

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.1788662 &   0.1449433 &  \textbf{0.3238095}\\ 
  \hline
  rond &    0.2104309 &0.1705215 & \textbf{0.3809524}  \\ 
    \hline
  triangle &  0.1630839 & 0.1321542 &   \textbf{0.2952381}\\
  \hline
  \texttt{total} &  \textbf{0.552381} & \textbf{0.447619} & 1
  \end{array}$$

C'est le tableau des **fréquences théoriques**.


## Tableau des effectifs théoriques

On l'obtient en multipliant les fréquences théoriques par la taille de la population observée (ici 105)


$$\begin{array}{c|c|c}
   & blanc & noir \\ 
   \hline
carré &  18.78095 &  15.21905 \\ 
  \hline
  rond &   22.09524 & 17.90476    \\ 
    \hline
  triangle & 17.12381 & 13.87619   \\
  \end{array}$$

<font size="4">
N.B. Il n'est pas nécessaire d'arrondir les effectifs théoriques
</font>


## Calcul du Chi 2


C'est la somme, pour chaque case du tableau de contingence,  des écarts carrés entre  effectif observé et effectif théorique, divisés par l'effectif théorique.

Soient $T^{obs}$ le tableau des effectifs observés, $T^{theo}$ le tableau des effectifs théoriques, 


<center>
$\chi^2 =  \sum_{i,j}  \frac{( T^{obs}_{i,j} -  T^{theo}_{i,j})^2}{T^{obs}_{i,j}}$       
</center>

ici : $\chi^2 = 26.30329$


## Interprétation du Chi 2 

Il faut comparé la valeur du $\chi^2$ calculée avec la **valeur critique** qu'on trouve   dans une **table de loi de Student** (ou table de loi du chi 2).

C'est un tableau à double entrée : une **valeur de quantile**, et un **degré de liberté**.

On peut considérer que la valeur de quantile est le pourcentage d'erreur qu'on s'autorise de faire.  On prend souvent **5%** :  la colonne 1-0.05 =  0.95

Le degré de liberté est obtenu en calculant la valeur $(nb\_lignes - 1)*(nb\_colonnes -1)$.

Dans notre exemple , le degré de liberté est 2*1 = 2


## Table de loi de Student 

<center>
<img src="table.loi.de.student.png", width=100%></img>
</center>


## Interprétation du Chi 2 


D'après le tableau de la loi de Student , la valeur critique pour un test avec 5% de chances de se tromper est un degré de liberté de 2 vaut 4.303.

Si la valeur calculée du $\chi^2$ est **supérieure** à la valeur critique, on **rejette** $H_0$.



Pour notre exemple: On conserve $H_0$, i.e. les deux variables sont **dépendantes**,  car $\chi ^2 \approx 26 > 4.303$

Interprétation: «la forme est liée à la couleur dans cette population, nous pouvons l'affirmer avec un risque d'erreur d'au moins  5%»


## Les étapes du $\chi ^2$

- Tableau de contingence
- Sommes marginales 
    - Calcul des fréquences observées 
    - Calcul des fréquences théoriques
- Tableau d'effectifs théoriques
- Calcul de la valeur du test 
- Comparaison avec les valeurs de la table de Student 



# Lien entre une variable qualitative et une variable quantitative.


## Représentation graphique.

Pas de moyen **simple** de calculer le lien entre une variable qualitative et une variable quantitative.
  
  - corrélation de rang 
  - régression logistique 
  - analyse de la variance (ANOVA)
  

La variable qualitative sert de **catégorie**, on fait varier la représentation graphique de la variable quantitative suivant cette catégorie.

2 possibilités :

  - boîtes à moustaches
  - superposition d' histogrammes / densités 

## Les boîtes à moustaches 

"Boîte à moustaches", aussi appellées "boxplot", montre les quartiles d'une variable

Quartile = «quantile de 4», valeurs qui séparent une variable quantitative en quarts, i.e. paquets d'un quart de l'effectif

- Premier quartile : sépare les 25% inférieurs des valeurs
- Second quartile : **mediane**, sépare les 50% inférieurs des valeurs
- troisième quartile : sépare les 75% inférieurs des valeurs


Dans R, fonction `quantile` 

## Boxplot et Distribution


```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.width=8, fig.height=6}
library(ggplot2)
library(cowplot)


n <-  4000
xval <-  rbeta(n, shape1 = 2, shape2 = 5) * 1000
mydf <-  data.frame(x=xval)
xmean <-  mean(xval)
xsd <-  sd(xval)

xlabels <- data.frame(
  x = c(quantile(mydf$x)),
  y = c(rep(330,5) ),
  text = paste0(c("","Q1: ", "Q2: ", "Q3: ", ""),names(quantile(mydf$x)))
)
distriPlot <-  ggplot(mydf, aes(x=x))+
  geom_histogram(bins = 50, fill="#0FAF96", color="grey")+
  geom_vline(data = xlabels, xintercept = xlabels$x, linetype=c(2,2,1,2,2))+
  geom_label(data= xlabels, aes(x=x,y=y,label=text) )+
  xlab("X")+
  ylab("Effectif")

# Marginal density plot of x (top panel) and y (right panel)
yplot <- ggplot(mydf, aes(y = xval))+
  geom_boxplot(outlier.alpha = 0.2)+
  coord_flip()+
  xlab("")+
  ylab("")+
  theme_void()

#assemblage
plot_grid(yplot, distriPlot,  ncol = 1, align = "hv",scale = c(1,1), 
          rel_widths = c(1, 1), rel_heights = c(1, 3))
```



## Boxplot par catégories


Principe : on trace un boxplot par modalité de la variable qualitative.

E.g. : consommation de véhicules par type (dataset `mpg` de R)


```{r, echo=FALSE, warning=FALSE}
p <- ggplot(mpg, aes(class, hwy))
p + geom_boxplot(outlier.alpha = 0.2) +xlab("type de véhicule (var. qualitative)") +ylab("consommation (var. quantitative")+theme_light()
```


# Références supplémentaires


## Refs


Cours complet sur les modèles linéaires : [https://www.math.univ-toulouse.fr/~barthe/M1modlin/poly.pdf]


Interprétation des graphique de `lm` en R : [https://data.library.virginia.edu/diagnostic-plots/]



